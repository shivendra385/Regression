{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oldue44xsim"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression\n",
        " -> A statistical method that models the relationship between a single independent variable (X) and a continuous dependent variable (Y).\n",
        "2.  What are the key assumptions of Simple Linear Regression\n",
        " -> Key assumptions:\n",
        "    - Linearity between X and Y\n",
        "    - Independence of observations\n",
        "    - Homoscedasticity (constant variance)\n",
        "    - Normality of residuals\n",
        "    - No multicollinearity\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c\n",
        "  -> Coefficient m (slope): Represents the change in Y for a one-unit change in X, while holding all other variables constant.\n",
        "4. What does the intercept c represent in the equation Y=mX+c\n",
        " ->Intercept c: Represents the value of Y when X is equal to 0.\n",
        "5. How do we calculate the slope m in Simple Linear Regression\n",
        " ->Calculating slope m: The slope is calculated using the formula: m = Σ[(xi - x̄)(yi - ȳ)] / Σ(xi - x̄)²\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression\n",
        " -> Least squares method: A method used to estimate the parameters of a linear regression model by minimizing the sum of the squared residuals.\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        " -> R² interpretation: Measures the proportion of variance in Y that is explained by X. A higher R² indicates a better fit of the model.\n",
        "8. What is Multiple Linear Regression\n",
        " -> Multiple Linear Regression: A statistical method that models the relationship between multiple independent variables (X1, X2, ...) and a continuous dependent variable (Y).\n",
        "9. What is the main difference between Simple and Multiple Linear Regression\n",
        " -> Simple Linear Regression has one independent variable, while Multiple Linear Regression has multiple independent variables\n",
        "10. What are the key assumptions of Multiple Linear Regression\n",
        " -> Key assumptions:\n",
        "    - Linearity between each X and Y\n",
        "    - Independence of observations\n",
        "    - Homoscedasticity\n",
        "    - Normality of residuals\n",
        "    - No multicollinearity\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "->  A condition where the variance of residuals varies across levels of X. It can lead to biased standard errors and incorrect conclusions.\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        " -> Improving model with multicollinearity: Techniques include removing correlated variables, using dimensionality reduction, or regularization methods like Lasso or Ridge regression.\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models\n",
        " ->Transforming categorical variables: Techniques include one-hot encoding, label encoding, or binary encoding.\n",
        "14. What is the role of interaction terms in Multiple Linear Regression\n",
        " -> Interaction terms: Allow the effect of one independent variable on the dependent variable to depend on the level of another independent variable.\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        " -> Intercept interpretation: In Multiple Linear Regression, the intercept represents the value of Y when all X variables are equal to 0.\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        " -> Slope significance: The slope represents the change in Y for a one-unit change in X. A significant slope indicates a strong relationship between X and Y.\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "->Intercept context: The intercept provides context for the relationship between variables by representing the baseline value of Y.\n",
        "18. What are the limitations of using R² as a sole measure of model performance\n",
        "->Limitations of R²: R² does not account for model complexity, and a high R² does not necessarily mean the model is good.\n",
        "19. How would you interpret a large standard error for a regression coefficient\n",
        "-> Interpreting standard error: A large standard error for a regression coefficient indicates that the estimate is imprecise.\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        " -> Identifying heteroscedasticity: Heteroscedasticity can be identified using residual plots, such as a plot of residuals vs. fitted values.\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²\n",
        " -> High R² but low adjusted R²: This indicates that the model is overfitting, and the high R² is due to the large number of predictors.\n",
        "22. Why is it important to scale variables in Multiple Linear Regression\n",
        " ->Scaling variables: Scaling variables can improve model performance and interpretation, especially when variables have different units or scales.\n",
        "23. What is polynomial regression\n",
        " ->Polynomial Regression: A type of regression analysis where the relationship between the independent variable(s) and dependent variable is modeled using a polynomial equation.\n",
        "24. How does polynomial regression differ from linear regression\n",
        " -> Difference from Linear Regression: Polynomial regression differs from linear regression in that it allows for non-linear relationships between the independent variable(s) and dependent variable.\n",
        "25. When is polynomial regression used\n",
        " ->Polynomial regression is used when the relationship between the independent variable(s) and dependent variable is non-linear, and a linear model is not sufficient to capture the underlying pattern.\n",
        "26. What is the general equation for polynomial regression\n",
        " -> The general equation for polynomial regression is: y = β0 + β1x + β2x² + … + βnx^n + ε\n",
        "27. Can polynomial regression be applied to multiple variables\n",
        " ->Yes, polynomial regression can be applied to multiple variables, and it is known as multivariate polynomial regression.\n",
        "28. What are the limitations of polynomial regression\n",
        " -> Limitations:\n",
        "    - Overfitting: Polynomial regression models can easily overfit the data, especially when the degree of the polynomial is high.\n",
        "    - Computational complexity: Polynomial regression models can be computationally expensive to fit, especially for large datasets.\n",
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        " -> Evaluating Model Fit: Methods to evaluate model fit include:\n",
        "    - Cross-validation\n",
        "    - Mean Squared Error (MSE)\n",
        "    - Coefficient of Determination (R²)\n",
        "    - Visual inspection of residual plots\n",
        "\n",
        "30. Why is visualization important in polynomial regression\n",
        " -> Visualization: Visualization is important in polynomial regression to:\n",
        "    - Understand the relationship between the independent variable(s) and dependent variable\n",
        "    - Identify non-linear patterns\n",
        "    - Check for overfitting or underfitting\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        " ->Polynomial regression can be implemented in Python using libraries such as:\n",
        "    - scikit-learn: Provides a PolynomialFeatures class to generate polynomial features, and a LinearRegression class to fit the model.\n",
        "    - numpy: Provides functions for polynomial fitting and evaluation.\n",
        "    - statsmodels: Provides a Polynomial Regression class to fit polynomial regression models.\n",
        "\n",
        "Example code:\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)"
      ],
      "metadata": {
        "id": "AR4hrSJCyRgb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zqcQL_uU2nif"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}